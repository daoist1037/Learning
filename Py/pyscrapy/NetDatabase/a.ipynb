{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html2 = urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\")\n",
    "bsObj = BeautifulSoup(html2.read(),'lxml')\n",
    "namelist = bsObj.findAll('span',{\"class\":\"green\"})\n",
    "for name in namelist:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "for child in bsObj.find(\"table\",{\"id\":\"giftList\"}).children:\n",
    "    # 子标签\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "for child in bsObj.find(\"table\",{\"id\":\"giftList\"}).descendants:\n",
    "    # descendants 后代标签\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "for sibling in bsObj.find(\"table\",{\"id\":\"giftList\"}).tr.next_siblings:\n",
    "    # 所有兄弟标签\n",
    "    print(sibling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "print(bsObj.find(\"img\",{\"src\":\"../img/gifts/img1.jpg\"}).parent.previous_sibling.get_text())\n",
    "# 该标签的父亲的前一个兄弟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "images = bsObj.findAll(\"img\",{\"src\":re.compile(\"\\.\\.\\/img\\/gifts/img.*\\.jpg\")})\n",
    "for image in images:\n",
    "    print(image[\"src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "images = bsObj.findAll(\"img\",{\"src\":re.compile(\"\\.\\.\\/img\\/gifts/img.*\\.jpg\")})\n",
    "for image in images:\n",
    "    for attrs in image.attrs:\n",
    "        # image.attrs获得该标签的所有属性\n",
    "        # image.attrs['src']获得属性列表中src的内容\n",
    "        print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "someTags = bsObj.findAll(lambda tag: len(tag.attrs)==2)\n",
    "# lambda表达式\n",
    "for tag in someTags:\n",
    "    print(tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "# html = urlopen(\"http://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
    "html = urlopen(\"https://baike.baidu.com/\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "# for link in bsObj.findAll(\"a\"):\n",
    "#     if 'href' in link.attrs:\n",
    "#         print(link.attrs['href'])\n",
    "print(len(bsObj.findAll(\"a\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.randint(0,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('../result.csv','a',newline='') as f:\n",
    "    # newline='' 让写入时不会多空行\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('num 1','num 2','num 3'))\n",
    "    for i in range(10):\n",
    "        writer.writerow((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('../result.csv','a',newline='') as f:\n",
    "    # newline='' 让写入时不会多空行\n",
    "    writer = csv.DictWriter(f,fieldnames=['name','age','wage'])\n",
    "    # 以字典形式写入，可以指定标题头，写入的时候以字典写入（增强可读性）\n",
    "    # 在每次打开csv文件写入都必须要指定fielenames，如果以字典写入\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'name':'王五', 'age':23, 'wage':1800})\n",
    "    # for i in range(10):\n",
    "    #     writer.writerow((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "textPage = urlopen(\"http://www.pythonscraping.com/pages/warandpeace/chapter1.txt\")\n",
    "print(textPage.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "textPage = urlopen(\"http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt\")\n",
    "print(str(textPage.read(), 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "content = bsObj.find(\"div\", {\"id\":\"mw-content-text\"}).get_text()\n",
    "content = bytes(content, \"UTF-8\")\n",
    "content = content.decode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import StringIO\n",
    "import csv\n",
    "data = urlopen(\"http://pythonscraping.com/files/MontyPythonAlbums.csv\").read().decode('ascii', 'ignore')\n",
    "dataFile = StringIO(data)\n",
    "# 可以将字符串封装成文件形式\n",
    "# csvReader = csv.reader(dataFile)\n",
    "# for row in csvReader:\n",
    "#     print(row)  \n",
    "dictReader = csv.DictReader(dataFile)\n",
    "print(dictReader.fieldnames)\n",
    "for row in dictReader:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "wordFile = urlopen(\"http://pythonscraping.com/pages/AWordDocument.docx\").read()\n",
    "wordFile = BytesIO(wordFile)\n",
    "document = ZipFile(wordFile)\n",
    "xml_content = document.read('word/document.xml')\n",
    "print(xml_content.decode('utf-8'))\n",
    "\n",
    "wordObj = BeautifulSoup(xml_content.decode('utf-8'))\n",
    "textStrings = wordObj.findAll(\"w:t\")\n",
    "for textElem in textStrings:\n",
    "    print(textElem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from pdfminer.pdfinterp import PDFResourceManager, process_pdf\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from io import open\n",
    "def readPDF(pdfFile):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    process_pdf(rsrcmgr, device, pdfFile)\n",
    "    device.close()\n",
    "    content = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return content\n",
    "pdfFile = urlopen(\"http://pythonscraping.com/pages/warandpeace/chapter1.pdf\")\n",
    "outputString = readPDF(pdfFile)\n",
    "print(outputString)\n",
    "pdfFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "baidu = webdriver.Chrome()\n",
    "baidu.get('https://www.baidu.com/s?wd=华中科技大学')\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "params = {'firstname': 'Ryan', 'lastname': 'Mitchell'}\n",
    "r = requests.post(\"http://pythonscraping.com/files/processing.php\", data=params)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "files = {'uploadFile': open('../files/Python-logo.png', 'rb')}\n",
    "# 提交文件\n",
    "r = requests.post(\"http://pythonscraping.com/pages/processing2.php\",\n",
    "files=files)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "params = {'username': 'Ryan', 'password': 'password'}\n",
    "r = requests.post(\"http://pythonscraping.com/pages/cookies/welcome.php\", params)\n",
    "print(\"Cookie is set to:\")\n",
    "print(r.cookies.get_dict())\n",
    "print(\"-----------\")\n",
    "print(\"Going to profile page...\")\n",
    "r = requests.get(\"http://pythonscraping.com/pages/cookies/profile.php\",\n",
    "cookies=r.cookies)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "session = requests.Session()\n",
    "# 针对经常暗自调整cookie的网址\n",
    "params = {'username': 'username', 'password': 'password'}\n",
    "s = session.post(\"http://pythonscraping.com/pages/cookies/welcome.php\", params)\n",
    "print(\"Cookie is set to:\")\n",
    "print(s.cookies.get_dict())\n",
    "print(\"-----------\")\n",
    "print(\"Going to profile page...\")\n",
    "s = session.get(\"http://pythonscraping.com/pages/cookies/profile.php\")\n",
    "print(s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import AuthBase\n",
    "# Requests 库有一个auth 模块专门用来处理HTTP 认证\n",
    "from requests.auth import HTTPBasicAuth\n",
    "auth = HTTPBasicAuth('ryan', 'password')\n",
    "r = requests.post(url=\"http://pythonscraping.com/pages/auth/login.php\", auth=\n",
    "auth)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "# 当界面中某个元素不存在的时候再运行，与presence_of_element_located相反\n",
    "def waitForLoad(driver):\n",
    "    elem = driver.find_element_by_tag_name(\"html\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        if count > 20:\n",
    "            print(\"Timing out after 10 seconds and returning\")\n",
    "            return\n",
    "        time.sleep(.5)\n",
    "        try:\n",
    "            elem == driver.find_element_by_tag_name(\"html\")\n",
    "        except StaleElementReferenceException:\n",
    "            return\n",
    "driver = webdriver.PhantomJS(executable_path='<Path to Phantom Js>')\n",
    "driver.get(\"http://pythonscraping.com/pages/javascript/redirectDemo1.html\")\n",
    "waitForLoad(driver)\n",
    "print(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "session = requests.Session()\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "url = \"https://www.whatismybrowser.com/developers/what-http-headers-is-my-browser-sending\"\n",
    "req = session.get(url, headers=headers)\n",
    "bsObj = BeautifulSoup(req.text)\n",
    "print(bsObj.find(\"table\",{\"class\":\"table-striped\"}).get_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import subprocess\n",
    "def cleanFile(filePath, newFilePath):\n",
    "    image = Image.open(filePath)\n",
    "    # 对图片进行阈值过滤，然后保存\n",
    "    image = image.point(lambda x: 0 if x<143 else 255)\n",
    "    # 如果 x<143 则x=0，否则x=255\n",
    "    image.save(newFilePath)\n",
    "    # 调用系统的tesseract命令对图片进行OCR识别\n",
    "    subprocess.call([\"tesseract\", newFilePath, \"output\"],shell=True)\n",
    "    # OCR  \n",
    "    # run()和call()\n",
    "    # 打开文件读取结果\n",
    "    outputFile = open(\"output.txt\", 'r')\n",
    "    print(outputFile.read())\n",
    "    outputFile.close()\n",
    "cleanFile(\"text.png\", \"text_2_clean.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://pythonscraping.com\")\n",
    "driver.implicitly_wait(1)\n",
    "# 隐式等待,当使用了隐士等待执行测试的时候，如果 WebDriver没有在 DOM中找到元素，将继续等待，超出设定时间后则抛出找不到元素的异常,换句话说，当查找元素或元素并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是0\n",
    "print(driver.get_cookies())\n",
    "# 查看cookie 得到列表\n",
    "# 还可以调用delete_cookie()、add_cookie() 和delete_all_cookies() 方法来处理cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "# from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "r = webdriver.Chrome()\n",
    "r.get('http://pythonscraping.com/pages/javascript/draggableDemo.html')\n",
    "\n",
    "time.sleep(5)\n",
    "print(driver.find_element_by_id(\"message\").text)\n",
    "elemnt = driver.find_element_by_id(\"draggable\")\n",
    "target = driver.find_element_by_id(\"div2\")\n",
    "actioactionstionChains(driver)\n",
    "actioactions_and_drop(element, target).perform()\n",
    "print(driver.find_element_by_id(\"message\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37064bitbaseconda3c4173f0a6f14868888853df358d8835",
   "display_name": "Python 3.7.0 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}